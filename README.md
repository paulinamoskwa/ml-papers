




# Language
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Mixture of Experts Explained](https://huggingface.co/blog/moe) (MoE) and [Mixtral of experts](https://mistral.ai/news/mixtral-of-experts/) (Mixtral 8x7B)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Modeling Is Compression](https://arxiv.org/abs/2309.10668)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Mistral 7B](https://arxiv.org/abs/2310.06825)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442v2)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (Chinchilla)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2021-08a045?logo=date&style=flat-square)</sub> &nbsp;
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2020-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2019-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (T5) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2019-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2019-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) (GPT-2) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2018-08a045?logo=date&style=flat-square)</sub> &nbsp;
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2018-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (GPT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2018-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) (ELMo) 
<br>

# Vision and Image Generation
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/01.2024-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Grounded Segment Anything](https://arxiv.org/abs/2401.14159) (Grounded SAM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-blue?logo=date&style=flat-square)</sub> &nbsp;
[Segment Anything](https://arxiv.org/abs/2304.02643) (SAM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-blue?logo=date&style=flat-square)</sub> &nbsp;
[Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/01.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (Stable Diffusion) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857) (GLIP)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) (MAE)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Simple and Efficient Design For Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) (SegFormer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2021-blue?logo=date&style=flat-square)</sub> &nbsp;
[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) (DINO)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2021-blue?logo=date&style=flat-square)</sub> &nbsp;
[Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) (Swin Transformer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2) (DeiT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) (ViT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2020-blue?logo=date&style=flat-square)</sub> &nbsp; 
[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) (DETR)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2017-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Mask R-CNN](https://arxiv.org/abs/1703.06870) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2015-blue?logo=date&style=flat-square)</sub> &nbsp;
[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (YOLOv1)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Fast R-CNN](https://arxiv.org/abs/1504.08083) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2013-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) (R-CNN) 
<br>

# General AI
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2023-red?logo=date&style=flat-square) </sub> &nbsp;
[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2023-08a045?logo=date&style=flat-square) </sub> &nbsp;
[Attention Sinks in Transformers for endless fluent generation](https://huggingface.co/blog/tomaarsen/attention-sinks)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square) </sub> &nbsp; 
[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2023-red?logo=date&style=flat-square) </sub> &nbsp;
[FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2022-red?logo=date&style=flat-square) </sub> &nbsp;
[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2021-red?logo=date&style=flat-square) </sub> &nbsp;
[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2021-red?logo=date&style=flat-square) </sub> &nbsp;
[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (CLIP)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2017-red?logo=date&style=flat-square) </sub> &nbsp; 
[Attention is All You Need](https://arxiv.org/abs/1706.03762) (Transformer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2014-red?logo=date&style=flat-square) </sub> &nbsp;
[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661v1) (GAN)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2013-red?logo=date&style=flat-square) </sub> &nbsp;
[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (VAE)
<br>

<br>

----

<br>

|Schema|Summary|
|:-:|-|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/db274f6f-0c47-4c42-af52-70bc681d983c)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &thinsp; [Chinchilla](https://arxiv.org/abs/2203.15556) </h3> *Given a fixed FLOPs (compute) budget, how should one scale the model size and the number of training tokens?* Three different approaches are proposed in this paper, all reaching the same conclusion: the number of parameters and the number of training tokens should be increased equally with more compute. This is in contrast with previous work on scaling laws for LLMs, which led to a trend of increasing model size without properly scaling the number of training tokens. To validate their analysis, the authors trained a modified version of a huge non-computational-optimal model, *Gopher*, making it smaller (70B params) and training it on more tokens (1.4T tokens): *Chinchilla*. *Chinchilla* outperforms *Gopher*. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/2e9a9ebb-08de-46fc-9438-d1a45e449893)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &thinsp; [InstructGPT (RLHF with GPT-3)](https://arxiv.org/abs/2203.02155) </h3> Introducing InstructGPT, a GPT-3 model fine-tuned on a reward signal learned from human data. The goal is to align a LLM with human preferences. Given a prompt, the LLM produces multiple outputs, and human preferences are collected and used to train a reward model. The reward model is then used as a signal for fine-tuning the initial LLM: when given a prompt and the model's initial output, the prediction of the next token involves the reward model's opinion. This strategy introduces human bias to the text data distribution. Standard LLMs learn to precisely model the distribution of data. In contrast, the RL component here is not doing distribution matching, but mode seeking (identifies and favours the most preferred outputs, rather than capturing the entire distribution of possibilities). The focus on mode seeking sacrifices diversity in exchange of reliable generations. ([*relevant previous work*](https://arxiv.org/abs/2009.01325))<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/be40875b-ad73-4416-9319-2637e7339d91)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2021-08a045?logo=date&style=flat-square)</sub> &thinsp; [LoRA](https://arxiv.org/abs/2106.09685)</h3> Fine-tuning LLMs is expensive due to their size, as all their parameters (and their gradients) must be loaded into the GPU. LoRA (Low-Rank Adaptation) offers a parameter-efficient alternative by freezing the original weights and introducing a separate set of weights for fine-tuning. These fine-tuning weights are viewed as perturbation of the original weights, ensuring no increase in memory usage during inference. LoRA achieves efficiency by employing low-rank decomposition, representing the new weight matrices as the product of two matrices containing fewer parameters than the original matrix. For a transformer layer represented by weight matrix *W*, LoRA generates a new weight matrix *Î”W* which is then decomposed by lower-rank matrices *A* and *B*. A crucial hyperparameter in LoRA is the rank of the decomposition, *r*: if too low, the fine-tuning won't be efficient, if too high, computational resources may be wasted. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a7119391-e565-45f1-a646-a12409c6aafd)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2020-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT-3](https://arxiv.org/abs/2005.14165)</h3>Previous language models, such as BERT, have a pre-training and fine-tuning stages. During fine-tuning, the model is trained via repeated gradient updates using a large corpus of example tasks. In contrast, GPT-2 employs single-step training, integrating the task description in the input. GPT-3 builds upon this approach, emphasizing scaling. GPT-3 is still a decoder-only architecture (with way more parameters than GPT-2). It is still trained in a generative way, and then, it is used in a in-context learning framework (as a few shot learner). In-context learning involves presenting examples of desired behaviour directly within the input. Unlike fine-tuning, in-context learning does not update the gradient. GPT-3 exceeded state-of-the-art.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a38f126d-5438-4417-8c4c-e86b15a5b847)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [T5](https://arxiv.org/abs/1910.10683)</h3> The (long) paper introduces T5 (text-to-text transfer transformer), a novel approach that frames all NLP tasks as a text-to-text problem. In this framework, the model learns how to map one input text to a target text, regardless of the specific task (translation, summarization, classification, etc). This simplifies the pre-training process as all tasks can be framed as text generation tasks. T5 model architecture is an encoder-decoder transformer. T5 is pre-trained on multiple tasks simultaneously using a unified objective (helps with generalization).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4753c2ec-9eb5-44ab-9622-14e888dccd3c)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/07.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [RoBERTa](https://arxiv.org/abs/1907.11692) </h3> RoBERTa (robustly optimized BERT approach) optimizes the pre-training of BERT. In particular, the paper proposes (1) to train the model longer, with bigger batches, over more data, (2) to remove the next sentence prediction objective (no gains), (3) to train on longer sequences, (4) to dynamically change the masking pattern applied to training data, and (5) to create a large new dataset (CC-NEWS) of comparable size to other privately used datasets. About (4): BERT masks randomly 15% of each sentence, but if the sentence is encountered twice then it'll be masked in the same way. RoBERTa instead implements an on-the-fly mask generation.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/73ee36cc-66a9-4166-a07e-fb0ae6a37554)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)</h3>Previous to this paper, the approach for task-specific language modeling was always unsupervised pre-training followed by supervised fine-tuning. GPT-2 introduces a new approach and becomes a zero-shot NLP task model, *i.e.* able to perform specific tasks without a change in architecture nor being trained on particular data. To achieve its generality, GPT-2 is trained to predict *p(output*\|*input, task)* rather than *p(output*\|*input)*. A training sample for translation is `("translate to french", english text, french text)`, and a sample for reading comprehension is `("answer the question", document, question, answer)`. How can this be enough? If the model is trained on enough (diverse) data, it is already implicitly learning a lot of different tasks.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/ef1cc2fc-fb75-46fe-8d74-0fd684068493)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [BERT](https://arxiv.org/abs/1810.04805) </h3>Introducing BERT (bidirectional encoder representation from transformers). While GPT is decoder-only, BERT relies on transformer's encoder. The attention layer in the encoder is not masked, so each token can attain to all the others (that's why BERT is bidirectional). While GPT is pre-trained in a generative way (predict the next token), BERT is pre-trained on two tasks: predict randomly masked words in the input (15% of tokens are randomly masked), and determine whether a pair of sentences is composed of two subsequent sentences. The loss function for pre-training is the combination of the two losses. Once pre-trained, BERT can be fine-tuned for downstream tasks.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/74a1df42-2b95-4ae7-aaae-773784ae6cb1)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)</h3> Presenting GPT (generative pre-training transformer), a method for pre-training and fine-tuning a language model using the transformer's decoder. Similar to ELMo, GPT generates contextualized word embeddings. However, instead of using bidirectional LSTMs, GPT relies on a stack of transformer decoder blocks, with positional information handled by positional embeddings. The network undergoes unsupervised pre-training, where it learns to generate the next word in a sequence. For supervised fine-tuning, the paper proposes various model architectures (head adaptations).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4e261010-0204-413a-9b67-7e9934e9c780)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [ELMo](https://arxiv.org/abs/1802.05365) </h3> Introducing a word embedding method that learns words representations based on their context in a sentence. ELMo (embeddings from language models) generates contextualized embeddings through the usage of deep bidirectional LSTMs. The same sentence goes through a stack of left-to-right and a stack of right-to-left LSTMs. Then, the left-to-right and right-to-left internal states are concatenated and (softmax-) weighted to form the final embedding for each word. <br><br>|
|||
|||
|||
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/e92441b7-dbac-4d89-886d-6297d1eaaf04)|<h3> &thinsp; <sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-blue?logo=date&style=flat-square)</sub> &thinsp; [SAM](https://arxiv.org/abs/2304.02643)</h3> Introducing SAM (Segment Anything Model), a zero-shot segmentation model based on user prompts (points, boxes, texts). Promptable segmentation is a new vision task introduced by this paper. SAM is trained on SA-1B, a new 11 million images dataset containing 1 billion segmentation masks. SAM takes as input an image and a prompt, and processes them with two independent encoders. The generated embeddings are then passed to a lightweight mask decoder that outputs three segmentation masks (one single mask creates issues with ambiguous prompts). In addition, SAM predicts also its own confidence score for each mask, *i.e.* what SAM thinks the IoU of the masks with the ground truths will be. This way, SAM is able to also have a confidence for its predictions. The mask with the highest *(true)* IoU with the ground truth is used to compute the loss, which is a combination of focal loss (focus on hard pixels), dice loss (variation of IoU), and MSE (between the actual IoU and the predicted one). <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/e3edde2b-e9d0-46c2-94b4-954a5eba9189)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2023-blue?logo=date&style=flat-square)</sub> &thinsp; [Grounding DINO](https://arxiv.org/abs/2303.05499)</h3> The paper combines DINO (DETR with Improved deNoising anchOr boxes) with GLIP to extend object detection to unseen categories based on human input (open set detection). While GLIP teaches the model to associate words with visual regions (language aware object detection), Grounding DINO is a (hybrid) self-supervised algorithm that learns to detect objects without explicit labels. Grounding DINO leverages unlabeled data in the form of image-text pairs. For each `(Image, Text)` pair, image and text features are extracted using separate backbones, and fed into a feature enhancer module for cross-modality feature fusion. A language-guided query selection module selects then the visual features that are more relevant to the input text. These visual features, along with the text features, are fed into a cross-modality decoder. The output queries (attention layers involved) of the latter are enhanced visual features used to predict object boxes. The model learns to adjust boxes with a localization loss, and to map them to the correct text with a contrastive loss. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/41f933d8-7421-4d90-8040-f705e5e54462)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/11.2022-blue?logo=date&style=flat-square)</sub> &thinsp; [OneFormer](https://arxiv.org/abs/2211.06220)</h3> Introducing OneFormer, the first multi-task universal image segmentation framework based on transformers. With a single, fixed architecture, OneFormer is a task-conditioned model for semantic, instance and panoptic segmentation. It is trained only once using a panoptic segmentation dataset, and outperforms models trained individually on each segmentation task. During training, one segmentation task is sampled and passed to the model as text input. Then, the panoptic ground truth is adapted to the sampled task. Inside OneFormer, two sets of queries are created: text queries (text-based representation for segments in the image) and object queries (image features extracted and processed by a transformer). A contrastive loss is used to align the text and object queries, inherently guiding the model to distinguish between tasks. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4a9e55f7-1552-4e69-9fae-e1958c98f15c)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [Stable Diffusion (Latent Diffusion Models)](https://arxiv.org/abs/2112.10752)</h3> A diffusion process progressively adds noise to an image until the image becomes a gaussian noise. A diffusion model (U-net) learns the reverse process, moving from gaussian noise back to the original image. This is achieved through incremental steps, with the model taking each time a noisy image as input and producing a less-noisy image as output. How does the text come into play? The text is encoded and concatenated to the input (noisy image) of the denoiser (U-net). Any form of conditioning can be encoded (text, image, etc). In addition to the concatenation, inside the U-net model the attention layers are able to attend to the conditioning tokens. This paper introduces LDMs (Latent Diffusion Models), a way to address the image-size limitation of diffusion models. LDMs first encode the image into a latent space, conduct the whole diffusion and denoising processes inside the latent space, and then decode back the output. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/84813450-1257-4240-945a-66f6aed31569)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [GLIP](https://arxiv.org/abs/2112.03857)</h3> Introducing GLIP (Grounded Language-Image Pretraining), a model which unifies object detection and phrase grounding (localizing an object in an image referred to by a natural language query) for learning object-level language aware visual representation. Object detection and phrase grounding are similar, as they both seek to localize objects and align them to semantic concepts. Any object detection model can be converted to a grounding model by replacing the object classification logits with the word-region alignment scores (dot product of the region visual features and the token language features). GLIP loss is the combination of box regression and region-word alignment. GLIP includes a deep fusion between image and text encoders, making the detection model language-aware. Cross-modality communication is achieved by the cross-modality multi-head attention module (X-MHA), where each head computes the context vectors of one modality by attending to the other modality. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/1812b415-33fa-4968-8b74-60ce44d24b1e)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/11.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [MAE](https://arxiv.org/abs/2111.06377)</h3> Inspired by NLP's BERT, this paper introduces MAE (Masked Autoencoders), a scalable and effective self-supervised approach for visual representation learning. In MAE, an image is divided into patches, and a random subset of these patches is masked. The visible patches are then fed into a transformer encoder. After the encoding, the masked patches are reintroduced, and a decoder learns to reconstruct the original image. Beyond pixel-level reconstruction, MAE learns powerful representations useful for downstream tasks. Both trained with self-supervised approaches, MAE differs from DINO since MAE learns representations by masking out certain parts of images and then learning to predict them, while DINO learns representations by creating pairs of images from the same source, but with different augmentations, and then mapping the pair to the same latent representation. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/dd96c1d3-5197-4f11-9f5c-c14e812b6c29)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [SegFormer](https://arxiv.org/abs/2105.15203)</h3> Introducing SegFormer (Segmentation with Transformer), a transformer-based architecture for semantic segmentation. After passing an image through multiple transformer encoder layers (where patches get aggregated after each layer), the representation of patches are decoded into a pixel-wise segmentation mask. This decoder head is a simple multilayer perceptron (MLP) that combines information from different encoder layers (hierarchical features from the encoder). While the encoder includes some downsampling at each step (merging of patches), the decoder takes care of the upsampling. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/48904623-23f0-4a01-8968-158a8612038c)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [MLP-Mixer](https://arxiv.org/abs/2105.01601) </h3> Presenting MLP-Mixer for image classification, an architecture for vision based on multi-layer perceptrons rather than convolutions or attention. MLP-Mixer takes as input an image divided into patches. Patches are pushed through a linear layer (to reduce dimensionality) and then are processed by a mixer layer, where multi-layer perceptrons are repeatedly applied across features channels (to process information independently for each patch, enhancing local features) and spatial locations (to process information across patches, capturing relationships between them). MLP-Mixer demonstrates competitive performance on image classification compared to CNN-models and ViT, while having a linear complexity in terms of sequence length (ViT has quadratic).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/153d6599-4039-40fd-b737-3a8b01980fc0)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/04.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [DINO](https://arxiv.org/abs/2104.14294) </h3> Introducing DINO (self-<b>di</b>stillation with <b>no</b> labels), a self-supervised training for ViT. DINO does not involve contrastive learning, only self-distillation. It consists of a teacher and a student, where the teacher is constructed as an exponential moving average of past student iterations. Given an image, an augmentation goes through the teacher, and a different augmentation goes through the student. DINO optimizes the student to learn the same representation produced by the teacher. More precisely, two augmentations _x1, x2_ of the image _x_ go through the student and the teacher. Both networks generate a representation for both images: _s1, s2, t1, t2_. DINO teaches the student to make _(s1, t2)_ similar, and _(s2, t1)_ similar. This <i>simple</i> strategy leads to features that contain object boundaries (accessible from the last `CLS` attention map) and perform well in representing images (k-NN classification without fine-tuning).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/2d8a465e-d052-49ce-b0ab-5167236f1090)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2021-blue?logo=date&style=flat-square)</sub> &thinsp; [Swin Transformer](https://arxiv.org/abs/2103.14030) </h3> Swin (Shifted windows) Transformer builds upon ViT by introducing a hierarchical method for processing images. Instead of fixed-size patches for the whole image, it starts with small patches, processes them through a transformer layer, groups (merges) the output, processes them again, and so on. Within the transformer layer, a shifted window self-attention is used to limit communication between patches, reducing computational complexity while still capturing essential dependencies. Each patch is only able to communicate with its neighbours. By iteratively processing and merging patches, Swin transformer better handles varying image sizes and fine-grained tasks (than ViT). <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a8dca8c8-f12d-4f0c-96d3-b311f71c1ede)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DeiT](https://arxiv.org/abs/2012.12877v2)</h3> ViT achieves state-of-the-art results in image classification by using more than 300M images for training (JFT-300M). This paper introduces DeiT (Data-efficient image Transformer), an encoder-only model that achieves competitive results being trained only on ImageNet. Convolutions already know how to *look* at images (patch by patch, with learnable filters), so they introduce a bias. Transformer are way more general, so they need to learn also how to handle images directly from the data. This necessitates large datasets. DeiT circumvents this need introducing a teacher-student strategy that relies on a distillation token, ensuring that the student learns from a CNN teacher. Why a CNN? CNNs possess prior knowledge about images, so they can learn from less data more easily than a transformer).  <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/e25cdddf-f7c2-4880-9837-7a5ce2e91413)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [ViT](https://arxiv.org/abs/2010.11929) </h3> Introducing ViT (Vision Transformer), an adaptation of the transformer for computer vision applications (image classification in this paper). While this is not the first paper to use transformers for computer vision (see DETR), it is the first work to completely discard CNNs. With ViT, an image is split into fixed-size patches, each of which is linearly embedded and combined with (learnable) positional embedding. The resulting sequence is then fed into a transformer encoder. For classification, an extra learnable classification token is added to the sequence. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/6709e2f5-67e4-47ca-9c38-80b719f91310)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DDPM](https://arxiv.org/abs/2006.11239)</h3> DDPM involves mapping a complex distribution of images into a simpler distribution (Gaussian noise) through a T-steps forward process, and then mapping it back through the backward process. The forward process is defined as Markov chain with Gaussian transition, causing also the backward process to have the same structure. The forward process is fixed, and we learn an approximation of the backward process. To learn the backward process, we maximize the log-likelihood of the data. As in VAE, we actually compute a lower bound and optimize for that. If we are able to create such a bridge (mapping) between the two distributions, then we can sample random noise and go to the original complex distribution to obtain an image. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/b9f660c7-351b-429a-b9d2-4196bfc54c40)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DETR](https://arxiv.org/abs/2005.12872)</h3> Introducing DETR (DEtection TRansformers) for object detection (and panoptic segmentation). DETR incorporates CNNs to initially process the input image and extract a *HxWxC* feature tensor. These features are seen as a sequence of *1x1xC* features, which are then passed to the transformer model. The transformer generates a sequence of box predictions and class probabilities. Boxes can be set as empty (no need for non-max suppression at the end). To align the output of the model with the ground truths, a matching loss is used. The model optimizes the IoU between predicted and gt boxes, and also the exact number of predictions (the model is penalized if it predicts less empty boxes than expected). <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/1b0665b6-9e19-4fe9-9b5c-e129f7d4fddb)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2017-blue?logo=date&style=flat-square)</sub> &thinsp; [Mask R-CNN](https://arxiv.org/abs/1703.06870) </h3> Mask R-CNN is an extension of Faster R-CNN that incorporates a pixel-level segmentation task alongside object detection. It introduces an additional branch in the network that predicts a (binary) segmentation masks for each detected object. Since the backbone network is shared for both tasks, Mask R-CNN efficiently produces accurate object masks while maintaining real-time performance. It enables precise instance segmentation, distinguishing between different object instances within the same class.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/cca1fdb5-50a5-4b6e-9067-97f9c593ebf1)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [SSD](https://arxiv.org/abs/1512.02325) </h3> Introducing single shot multiBox detector (SSD), a single-stage approach for object detection. The idea of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. To achieve high accuracy, SSD produces predictions from features maps of different scales.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/d3364d52-c53f-4e95-a663-3d9d71fd02d5)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [YOLO](https://arxiv.org/abs/1506.02640)</h3>Presenting YOLO, a single-stage appraoch for real-time object detection. YOLO places a grid over the input image and makes each cell responsible for predicting (1) _B_ bounding boxes, each associated with a confidence of the presence of an object within the boxes, (2) a conditional class probability. The bounding boxes are then associated with the class probabilities. A threshold is applied to select only the most confident boxes.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/73e8078c-035f-4bfa-bda8-b95eb8432f27)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [Faster R-CNN](https://arxiv.org/abs/1506.01497) </h3> In R-CNN, region of interest (RoIs) are generated using selective search and processed individually by a CNN. Fast R-CNN improves upon this by processing the entire image just once through the CNN, but still uses selective search. Faster R-CNN takes a step further by directly generating RoIs from the features produced by the CNN. Once the RoIs are generated, the subsequent steps remain the same as in Fast R-CNN. Even if it uses the same features, the method remains a two-stages method.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/c98d9bb9-c8ce-4331-8c7d-ffdc9dde4fbd)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [U-Net](https://arxiv.org/abs/1505.04597) </h3>New architecture for semantic segmentation. U-Net consists of a contracting path (standard convolutional network) followed by an expansive path. The expansive path involves upsampling of the feature map ("up-convolutions") and concatenating it with the corresponding feature map from the contracting path. Combining high resolution features from the contracting path with the upsampled output makes possible to generate very detailed segmentations.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a82810e1-4d9c-4c85-8df7-8b4382330dae)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/04.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [Fast R-CNN](https://arxiv.org/abs/1504.08083)</h3> Fast R-CNN is a significant improvement over the original R-CNN. R-CNN processes each region of interest (RoI) independently. Fast R-CNN, instead, processes the entire image in a single pass through the CNN. Then, it projects all the RoIs (produced on the original image by an external algorithm) onto the feature space. Each projected RoI is resized through the RoI pooling layer and further processed. A multi-task loss that combines classification and bounding box regression is used (unlike R-CNN, where each component is treated separately).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/f6984da5-97e7-4e0d-8559-952f1ff3eaa2)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/11.2013-blue?logo=date&style=flat-square)</sub> &thinsp; [R-CNN](https://arxiv.org/abs/1311.2524)</h3> Introducing R-CNN, a combination of region proposals and CNN features for object detection. R-CNN uses a region proposal algorithm (selective search) to first select some regions of interest. Each candidate region is resized to a fixed size and passed to the CNN, which then outputs a feature vector. The feature vector is fed into a collection of class-specific SVMs. Each SVM classifies whether the region (represented by the feature vector) contains an object of a specific class or not.<br><br>|
|||
|||
|||
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/400771ab-f012-40f1-a5dd-99c197df4129)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2022-red?logo=date&style=flat-square) </sub> &thinsp; [FlashAttention](https://arxiv.org/abs/2205.14135)</h3> Standard attention mechanism is slow (and memory-hungry). This paper identifies the bottleneck not in computation, but rather in data movement. Naive attention requires repeated reading/writing between GPU HBM (the primary memory source of the GPU) and SRAM (smaller and faster memory, where the computations are based). The paper introduces FlashAttention, an IO-aware **exact** attention algorithm that uses tiling to operate in chunks (instead of gigantic matrices) and kernel fusion to combine multiple operations into a single step, reducing the overall time spent in R/W. These techniques result in linear complexity in memory and allow waay faster computations, since they avoid redundant data movements and calculations. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/eb028989-348d-4b22-9031-b825add5d66f)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2021-red?logo=date&style=flat-square) </sub> &thinsp; [S4 (Structured State Space Models)](https://arxiv.org/abs/2111.00396)</h3> To address long-range dependencies in seq-to-seq tasks, SSMs (state space models) have been proposed. SSMs describe the evolution of a system over time by defining two sets of equations (state equations and observation equations). For SSMs to effectively memorize the input history (gain memory), the matrix describing the hidden state transition needs to belong to the HiPPO class. While this approach works well, it's computationally impractical. This paper proposes S4 (Structured State Space), which allows for a more practical solution by decomposing the HiPPO-form transition matrix of the hidden state into the sum of low-rank and normal matrices, and then diagonalizing the normal matrix. S4 is as efficient as existing methods, while achieving better performance on tasks involving long-range dependencies. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/26d0940f-e462-4fa0-be33-c8989fd6328d)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2021-red?logo=date&style=flat-square) </sub> &thinsp; [CLIP](https://arxiv.org/abs/2103.00020) </h3> CLIP (Contrastive Language-Image Pretraining) is trained to predict similarities between an image and a text. Unlike traditional methods of learning visual representations, CLIP leverages the freely available text data on the internet (400M image-text pairs, when ImageNet has 1.2M images). During training, CLIP takes a batch of images and their textual descriptions, embeds images and texts independently, and then learns to predict the correct pairing (contrastive approach). CLIP transfers to most tasks and is competitive as zero-shot against some trained (and fine-tuned) models. CLIP does not address the poor data efficiency of deep learning. Instead, it compensates by using a source of supervision that can be scaled to hundred of millions of training examples (learning about images from text). <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/50097be1-1443-4b4c-9d8a-8b598d909a2a)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2017-red?logo=date&style=flat-square) </sub> &thinsp; [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Transformer)</h3> Novel architecture for sequence-to-sequence tasks. Transformer relies exclusively on self-attention mechanisms, which enables each position in the input sequence to attend to all others simultaneously. This approach facilitates efficient capture of long-range dependencies and parallel computation. It achieves state-of-the-art performance in tasks like machine translation and text generation, setting a new standard for sequence modeling. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/b1d4a7d6-114f-429c-ae00-2467c4cd05db)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2014-red?logo=date&style=flat-square) </sub> &thinsp; [GAN](https://arxiv.org/abs/1406.2661v1)</h3> Introducing generative models trained via adversarial process. A generative model G captures the data distribution, while a discriminative model D estimates the probability that a given sample comes from the original data rather than being generated by G. The goal of G is to generate such good samples that D is fooled.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/28953bb3-69fe-46ef-8a6e-eb6724ca8da7)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2013-red?logo=date&style=flat-square) </sub> &thinsp; [VAE](https://arxiv.org/abs/1312.6114) </h3> Introducing variational autoencoders (VAEs), a class of generative models that extend the traditional autoencoder (AE) by incorporating probabilistic inference. While in AEs the encoder maps the input into a latent space vector, in VAEs the encoder maps the input into a latent space distribution. The decoder samples from the latent distribution and reconstructs the input. VAEs minimize the same loss function as AEs, with an additional regularization term that encourages the latent space to follow a prior distribution, typically a Gaussian.<br><br>|



