




# Language
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Mixture of Experts Explained](https://huggingface.co/blog/moe) (MoE) and [Mixtral of experts](https://mistral.ai/news/mixtral-of-experts/) (Mixtral 8x7B)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Modeling Is Compression](https://arxiv.org/abs/2309.10668)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Mistral 7B](https://arxiv.org/abs/2310.06825)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442v2)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (Chinchilla)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2021-08a045?logo=date&style=flat-square)</sub> &nbsp;
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2020-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2019-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (T5) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2019-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2019-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) (GPT-2) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2018-08a045?logo=date&style=flat-square)</sub> &nbsp;
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2018-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (GPT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2018-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) (ELMo) 
<br>

# Vision and Image Generation
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-blue?logo=date&style=flat-square)</sub> &nbsp;
[Segment Anything](https://arxiv.org/abs/2304.02643) (SAM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) (Grounded SAM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-blue?logo=date&style=flat-square)</sub> &nbsp;
[Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[DETRs with Collaborative Hybrid Assignments Training](https://arxiv.org/abs/2211.12860) (Co-DETR) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/01.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[Patches Are All You Need?](https://arxiv.org/abs/2201.09792) (ConvMixer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (Stable Diffusion) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) (MAE)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Simple and Efficient Design For Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) (SegFormer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2021-blue?logo=date&style=flat-square)</sub> &nbsp;
[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) (DINO)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2021-blue?logo=date&style=flat-square)</sub> &nbsp;
[Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) (Swin Transformer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2) (DEIT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) (ViT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2020-blue?logo=date&style=flat-square)</sub> &nbsp; 
[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) (DETR)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2017-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Mask R-CNN](https://arxiv.org/abs/1703.06870) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2015-blue?logo=date&style=flat-square)</sub> &nbsp;
[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (YOLOv1)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Fast R-CNN](https://arxiv.org/abs/1504.08083) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2013-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) (R-CNN) 
<br>

# General AI
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2023-red?logo=date&style=flat-square) </sub> &nbsp;
[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2023-08a045?logo=date&style=flat-square) </sub> &nbsp;
[Attention Sinks in Transformers for endless fluent generation](https://huggingface.co/blog/tomaarsen/attention-sinks)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square) </sub> &nbsp; 
[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2023-red?logo=date&style=flat-square) </sub> &nbsp;
[FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2022-red?logo=date&style=flat-square) </sub> &nbsp;
[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2021-red?logo=date&style=flat-square) </sub> &nbsp;
[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2021-red?logo=date&style=flat-square) </sub> &nbsp;
[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (CLIP)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2017-red?logo=date&style=flat-square) </sub> &nbsp; 
[Attention is All You Need](https://arxiv.org/abs/1706.03762) (Transformer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2014-red?logo=date&style=flat-square) </sub> &nbsp;
[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661v1) (GAN)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2013-red?logo=date&style=flat-square) </sub> &nbsp;
[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (VAE)
<br>




