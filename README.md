




# Language
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Mixture of Experts Explained](https://huggingface.co/blog/moe) (MoE) and [Mixtral of experts](https://mistral.ai/news/mixtral-of-experts/) (Mixtral 8x7B)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Modeling Is Compression](https://arxiv.org/abs/2309.10668)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Mistral 7B](https://arxiv.org/abs/2310.06825)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442v2)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (Chinchilla)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2022-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2021-08a045?logo=date&style=flat-square)</sub> &nbsp;
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2020-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2019-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (T5) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2019-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2019-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) (GPT-2) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2018-08a045?logo=date&style=flat-square)</sub> &nbsp;
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2018-08a045?logo=date&style=flat-square)</sub> &nbsp; 
[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (GPT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2018-08a045?logo=date&style=flat-square)</sub> &nbsp;
[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) (ELMo) 
<br>

# Vision and Image Generation
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2023-blue?logo=date&style=flat-square)</sub> &nbsp;
[Segment Anything](https://arxiv.org/abs/2304.02643) (SAM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Grounded Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) (Grounded SAM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2023-blue?logo=date&style=flat-square)</sub> &nbsp;
[Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[DETRs with Collaborative Hybrid Assignments Training](https://arxiv.org/abs/2211.12860) (Co-DETR) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/01.2022-blue?logo=date&style=flat-square)</sub> &nbsp;
[Patches Are All You Need?](https://arxiv.org/abs/2201.09792) (ConvMixer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (Stable Diffusion) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) (MAE)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2021-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Simple and Efficient Design For Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) (SegFormer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2021-blue?logo=date&style=flat-square)</sub> &nbsp;
[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) (DINO)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2021-blue?logo=date&style=flat-square)</sub> &nbsp;
[Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) (Swin Transformer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2) (DEIT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) (ViT)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2020-blue?logo=date&style=flat-square)</sub> &nbsp;
[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2020-blue?logo=date&style=flat-square)</sub> &nbsp; 
[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) (DETR)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/03.2017-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Mask R-CNN](https://arxiv.org/abs/1703.06870) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2015-blue?logo=date&style=flat-square)</sub> &nbsp;
[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (YOLOv1)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/04.2015-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Fast R-CNN](https://arxiv.org/abs/1504.08083) 
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/11.2013-blue?logo=date&style=flat-square)</sub> &nbsp; 
[Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) (R-CNN) 
<br>

# General AI
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2023-red?logo=date&style=flat-square) </sub> &nbsp;
[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2023-08a045?logo=date&style=flat-square) </sub> &nbsp;
[Attention Sinks in Transformers for endless fluent generation](https://huggingface.co/blog/tomaarsen/attention-sinks)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/09.2023-08a045?logo=date&style=flat-square) </sub> &nbsp; 
[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/07.2023-red?logo=date&style=flat-square) </sub> &nbsp;
[FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/05.2022-red?logo=date&style=flat-square) </sub> &nbsp;
[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/10.2021-red?logo=date&style=flat-square) </sub> &nbsp;
[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/02.2021-red?logo=date&style=flat-square) </sub> &nbsp;
[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (CLIP)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2017-red?logo=date&style=flat-square) </sub> &nbsp; 
[Attention is All You Need](https://arxiv.org/abs/1706.03762) (Transformer)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/06.2014-red?logo=date&style=flat-square) </sub> &nbsp;
[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661v1) (GAN)
<br>
<sub>&nbsp;&nbsp;![](https://img.shields.io/badge/12.2013-red?logo=date&style=flat-square) </sub> &nbsp;
[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (VAE)
<br>

----



|Schema|Summary|
|:-:|-|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a8dca8c8-f12d-4f0c-96d3-b311f71c1ede)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DeiT](https://arxiv.org/abs/2012.12877v2)</h3> .. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/e25cdddf-f7c2-4880-9837-7a5ce2e91413)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [ViT](https://arxiv.org/abs/2010.11929) </h3> Introducing ViT (Vision Transformer), an adaptation of the transformer for computer vision applications (image classification in this paper). While this is not the first paper to use transformers for computer vision (see DETR), it is the first work to completely discard CNNs. With ViT, an image is split into fixed-size patches, each of which is linearly embedded and combined with (learnable) positional embedding. The resulting sequence is then fed into a transformer encoder. For classification, an extra learnable classification token is added to the sequence. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/6709e2f5-67e4-47ca-9c38-80b719f91310)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DDPM](https://arxiv.org/abs/2006.11239)</h3> DDPM involves mapping a complex distribution of images into a simpler distribution (Gaussian noise) through a T-steps forward process, and then mapping it back through the backward process. The forward process is defined as Markov chain with Gaussian transition, causing also the backward process to have the same structure. The forward process is fixed, and we learn an approximation of the backward process. To learn the backward process, we maximize the log-likelihood of the data. As in VAE, we actually compute a lower bound and optimize for that. If we are able to create such a bridge (mapping) between the two distributions, then we can sample random noise and go to the original complex distribution to obtain an image. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a7119391-e565-45f1-a646-a12409c6aafd)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2020-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT-3](https://arxiv.org/abs/2005.14165)</h3>Previous language models, such as BERT, have a pre-training and fine-tuning stages. During fine-tuning, the model is trained via repeated gradient updates using a large corpus of example tasks. In contrast, GPT-2 employs single-step training, integrating the task description in the input. GPT-3 builds upon this approach, emphasizing scaling. GPT-3 is still a decoder-only architecture (with way more parameters than GPT-2). It is still trained in a generative way, and then, it is used in a in-context learning framework (as a few shot learner). In-context learning involves presenting examples of desired behaviour directly within the input. Unlike fine-tuning, in-context learning does not update the gradient. GPT-3 exceeded state-of-the-art.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/b9f660c7-351b-429a-b9d2-4196bfc54c40)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DETR](https://arxiv.org/abs/2005.12872)</h3> Introducing DETR (DEtection TRansformers) for object detection (and panoptic segmentation). DETR incorporates CNNs to initially process the input image and extract a *HxWxC* feature tensor. These features are seen as a sequence of *1x1xC* features, which are then passed to the transformer model. The transformer generates a sequence of box predictions and class probabilities. Boxes can be set as empty (no need for non-max suppression at the end). To align the output of the model with the ground truths, a matching loss is used. Moreover, the model optimizes the IoU between predicted and gt boxes, and also the exact number of predictions (the model is penalized if it predicts less empty boxes than expected). <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a38f126d-5438-4417-8c4c-e86b15a5b847)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [T5](https://arxiv.org/abs/1910.10683)</h3> The (long) paper introduces T5 (text-to-text transfer transformer), a novel approach that frames all NLP tasks as a text-to-text problem. In this framework, the model learns how to map one input text to a target text, regardless of the specific task (translation, summarization, classification, etc). This simplifies the pre-training process as all tasks can be framed as text generation tasks. T5 model architecture is an encoder-decoder transformer. T5 is pre-trained on multiple tasks simultaneously using a unified objective (helps with generalization).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4753c2ec-9eb5-44ab-9622-14e888dccd3c)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/07.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [RoBERTa](https://arxiv.org/abs/1907.11692) </h3> RoBERTa (robustly optimized BERT approach) optimizes the pre-training of BERT. In particular, the paper proposes (1) to train the model longer, with bigger batches, over more data, (2) to remove the next sentence prediction objective (no gains), (3) to train on longer sequences, (4) to dynamically change the masking pattern applied to training data, and (5) to create a large new dataset (CC-NEWS) of comparable size to other privately used datasets. About (4): BERT masks randomly 15% of each sentence, but if the sentence is encountered twice then it'll be masked in the same way. RoBERTa instead implements an on-the-fly mask generation.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/73ee36cc-66a9-4166-a07e-fb0ae6a37554)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)</h3>Previous to this paper, the approach for task-specific language modeling was always unsupervised pre-training followed by supervised fine-tuning. GPT-2 introduces a new approach and becomes a zero-shot NLP task model, *i.e.* able to perform specific tasks without a change in architecture nor being trained on particular data. To achieve its generality, GPT-2 is trained to predict *p(output*\|*input, task)* rather than *p(output*\|*input)*. A training sample for translation is `("translate to french", english text, french text)`, and a sample for reading comprehension is `("answer the question", document, question, answer)`. How can this be enough? If the model is trained on enough (diverse) data, it is already implicitly learning a lot of different tasks.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/ef1cc2fc-fb75-46fe-8d74-0fd684068493)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [BERT](https://arxiv.org/abs/1810.04805) </h3>Introducing BERT (bidirectional encoder representation from transformers). While GPT is decoder-only, BERT relies on transformer's encoder. The attention layer in the encoder is not masked, so each token can attain to all the others (that's why BERT is bidirectional). While GPT is pre-trained in a generative way (predict the next token), BERT is pre-trained on two tasks: predict randomly masked words in the input (15% of tokens are randomly masked), and determine whether a pair of sentences is composed of two subsequent sentences. The loss function for pre-training is the combination of the two losses. Once pre-trained, BERT can be fine-tuned for downstream tasks.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/74a1df42-2b95-4ae7-aaae-773784ae6cb1)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)</h3> Presenting GPT (generative pre-training transformer), a method for pre-training and fine-tuning a language model using the transformer's decoder. Similar to ELMo, GPT generates contextualized word embeddings. However, instead of using bidirectional LSTMs, GPT relies on a stack of transformer decoder blocks, with positional information handled by positional embeddings. The network undergoes unsupervised pre-training, where it learns to generate the next word in a sequence. For supervised fine-tuning, the paper proposes various model architectures (head adaptations).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4e261010-0204-413a-9b67-7e9934e9c780)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [ELMo](https://arxiv.org/abs/1802.05365) </h3> Introducing a word embedding method that learns words representations based on their context in a sentence. ELMo (embeddings from language models) generates contextualized embeddings through the usage of deep bidirectional LSTMs. The same sentence goes through a stack of left-to-right and a stack of right-to-left LSTMs. Then, the left-to-right and right-to-left internal states are concatenated and (softmax-) weighted to form the final embedding for each word. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/50097be1-1443-4b4c-9d8a-8b598d909a2a)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2017-red?logo=date&style=flat-square) </sub> &thinsp; [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Transformer)</h3> Introduces a novel architecture for sequence-to-sequence tasks. Transformer relies exclusively on self-attention mechanisms, which enables each position in the input sequence to attend to all others simultaneously. This approach facilitates efficient capture of long-range dependencies and parallel computation. It achieves state-of-the-art performance in tasks like machine translation and text generation, setting a new standard for sequence modeling. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/1b0665b6-9e19-4fe9-9b5c-e129f7d4fddb)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2017-blue?logo=date&style=flat-square)</sub> &thinsp; [Mask R-CNN](https://arxiv.org/abs/1703.06870) </h3> Mask R-CNN is an extension of Faster R-CNN that incorporates a pixel-level segmentation task alongside object detection. It introduces an additional branch in the network that predicts a (binary) segmentation masks for each detected object. Since the backbone network is shared for both tasks, Mask R-CNN efficiently produces accurate object masks while maintaining real-time performance. It enables precise instance segmentation, distinguishing between different object instances within the same class.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/cca1fdb5-50a5-4b6e-9067-97f9c593ebf1)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [SSD](https://arxiv.org/abs/1512.02325) </h3> Introduces single shot multiBox detector (SSD), a single-stage approach for object detection. The idea of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. To achieve high accuracy, SSD produces predictions from features maps of different scales.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/d3364d52-c53f-4e95-a663-3d9d71fd02d5)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [YOLO](https://arxiv.org/abs/1506.02640)</h3>Presents YOLO, a single-stage appraoch for real-time object detection. YOLO places a grid over the input image and makes each cell responsible for predicting (1) _B_ bounding boxes, each associated with a confidence of the presence of an object within the boxes, (2) a conditional class probability. The bounding boxes are then associated with the class probabilities. A threshold is applied to select only the most confident boxes.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/73e8078c-035f-4bfa-bda8-b95eb8432f27)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [Faster R-CNN](https://arxiv.org/abs/1506.01497) </h3> In R-CNN, region of interest (RoIs) are generated using selective search and processed individually by a CNN. Fast R-CNN improves upon this by processing the entire image just once through the CNN, but still uses selective search. Faster R-CNN takes a step further by directly generating RoIs from the features produced by the CNN. Once the RoIs are generated, the subsequent steps remain the same as in Fast R-CNN. Even if it uses the same features, the method remains a two-stages method.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/c98d9bb9-c8ce-4331-8c7d-ffdc9dde4fbd)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [U-Net](https://arxiv.org/abs/1505.04597) </h3>Introduces a new architecture for semantic segmentation. U-Net consists of a contracting path (standard convolutional network) followed by an expansive path. The expansive path involves upsampling of the feature map ("up-convolutions") and concatenating it with the corresponding feature map from the contracting path. Combining high resolution features from the contracting path with the upsampled output makes possible to generate very detailed segmentations.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a82810e1-4d9c-4c85-8df7-8b4382330dae)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/04.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [Fast R-CNN](https://arxiv.org/abs/1504.08083)</h3> Fast R-CNN is a significant improvement over the original R-CNN. R-CNN processes each region of interest (RoI) independently. Fast R-CNN, instead, processes the entire image in a single pass through the CNN. Then, it projects all the RoIs (produced on the original image by an external algorithm) onto the feature space. Each projected RoI is resized through the RoI pooling layer and further processed. A multi-task loss that combines classification and bounding box regression is used (unlike R-CNN, where each component is treated separately).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/b1d4a7d6-114f-429c-ae00-2467c4cd05db)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2014-red?logo=date&style=flat-square) </sub> &thinsp; [GAN](https://arxiv.org/abs/1406.2661v1)</h3> Introduces generative models trained via adversarial process. A generative model G captures the data distribution, while a discriminative model D estimates the probability that a given sample comes from the original data rather than being generated by G. The goal of G is to generate such good samples that D is fooled.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/28953bb3-69fe-46ef-8a6e-eb6724ca8da7)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2013-red?logo=date&style=flat-square) </sub> &thinsp; [VAE](https://arxiv.org/abs/1312.6114) </h3> Introduces variational autoencoders (VAEs), a class of generative models that extend the traditional autoencoder (AE) by incorporating probabilistic inference. While in AEs the encoder maps the input into a latent space vector, in VAEs the encoder maps the input into a latent space distribution. The decoder samples from the latent distribution and reconstructs the input. VAEs minimize the same loss function as AEs, with an additional regularization term that encourages the latent space to follow a prior distribution, typically a Gaussian.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/f6984da5-97e7-4e0d-8559-952f1ff3eaa2)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/11.2013-blue?logo=date&style=flat-square)</sub> &thinsp; [R-CNN](https://arxiv.org/abs/1311.2524)</h3> Introduces R-CNN, a combination of region proposals and CNN features for object detection. R-CNN uses a region proposal algorithm (selective search) to first select some regions of interest. Each candidate region is resized to a fixed size and passed to the CNN, which then outputs a feature vector. The feature vector is fed into a collection of class-specific SVMs. Each SVM classifies whether the region (represented by the feature vector) contains an object of a specific class or not.<br><br>|

.

.

.




|Schema|Summary|
|:-:|-|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a7119391-e565-45f1-a646-a12409c6aafd)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2020-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT-3](https://arxiv.org/abs/2005.14165)</h3>Previous language models, such as BERT, have a pre-training and fine-tuning stages. During fine-tuning, the model is trained via repeated gradient updates using a large corpus of example tasks. In contrast, GPT-2 employs single-step training, integrating the task description in the input. GPT-3 builds upon this approach, emphasizing scaling. GPT-3 is still a decoder-only architecture (with way more parameters than GPT-2). It is still trained in a generative way, and then, it is used in a in-context learning framework (as a few shot learner). In-context learning involves presenting examples of desired behaviour directly within the input. Unlike fine-tuning, in-context learning does not update the gradient. GPT-3 exceeded state-of-the-art.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a38f126d-5438-4417-8c4c-e86b15a5b847)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [T5](https://arxiv.org/abs/1910.10683)</h3> The (long) paper introduces T5 (text-to-text transfer transformer), a novel approach that frames all NLP tasks as a text-to-text problem. In this framework, the model learns how to map one input text to a target text, regardless of the specific task (translation, summarization, classification, etc). This simplifies the pre-training process as all tasks can be framed as text generation tasks. T5 model architecture is an encoder-decoder transformer. T5 is pre-trained on multiple tasks simultaneously using a unified objective (helps with generalization).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4753c2ec-9eb5-44ab-9622-14e888dccd3c)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/07.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [RoBERTa](https://arxiv.org/abs/1907.11692) </h3> RoBERTa (robustly optimized BERT approach) optimizes the pre-training of BERT. In particular, the paper proposes (1) to train the model longer, with bigger batches, over more data, (2) to remove the next sentence prediction objective (no gains), (3) to train on longer sequences, (4) to dynamically change the masking pattern applied to training data, and (5) to create a large new dataset (CC-NEWS) of comparable size to other privately used datasets. About (4): BERT masks randomly 15% of each sentence, but if the sentence is encountered twice then it'll be masked in the same way. RoBERTa instead implements an on-the-fly mask generation.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/73ee36cc-66a9-4166-a07e-fb0ae6a37554)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2019-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)</h3>Previous to this paper, the approach for task-specific language modeling was always unsupervised pre-training followed by supervised fine-tuning. GPT-2 introduces a new approach and becomes a zero-shot NLP task model, *i.e.* able to perform specific tasks without a change in architecture nor being trained on particular data. To achieve its generality, GPT-2 is trained to predict *p(output*\|*input, task)* rather than *p(output*\|*input)*. A training sample for translation is `("translate to french", english text, french text)`, and a sample for reading comprehension is `("answer the question", document, question, answer)`. How can this be enough? If the model is trained on enough (diverse) data, it is already implicitly learning a lot of different tasks.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/ef1cc2fc-fb75-46fe-8d74-0fd684068493)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [BERT](https://arxiv.org/abs/1810.04805) </h3>Introducing BERT (bidirectional encoder representation from transformers). While GPT is decoder-only, BERT relies on transformer's encoder. The attention layer in the encoder is not masked, so each token can attain to all the others (that's why BERT is bidirectional). While GPT is pre-trained in a generative way (predict the next token), BERT is pre-trained on two tasks: predict randomly masked words in the input (15% of tokens are randomly masked), and determine whether a pair of sentences is composed of two subsequent sentences. The loss function for pre-training is the combination of the two losses. Once pre-trained, BERT can be fine-tuned for downstream tasks.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/74a1df42-2b95-4ae7-aaae-773784ae6cb1)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)</h3> Presenting GPT (generative pre-training transformer), a method for pre-training and fine-tuning a language model using the transformer's decoder. Similar to ELMo, GPT generates contextualized word embeddings. However, instead of using bidirectional LSTMs, GPT relies on a stack of transformer decoder blocks, with positional information handled by positional embeddings. The network undergoes unsupervised pre-training, where it learns to generate the next word in a sequence. For supervised fine-tuning, the paper proposes various model architectures (head adaptations).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/4e261010-0204-413a-9b67-7e9934e9c780)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/02.2018-08a045?logo=date&style=flat-square)</sub> &thinsp; [ELMo](https://arxiv.org/abs/1802.05365) </h3> Introducing a word embedding method that learns words representations based on their context in a sentence. ELMo (embeddings from language models) generates contextualized embeddings through the usage of deep bidirectional LSTMs. The same sentence goes through a stack of left-to-right and a stack of right-to-left LSTMs. Then, the left-to-right and right-to-left internal states are concatenated and (softmax-) weighted to form the final embedding for each word. <br><br>|
|||
|||
|||
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a8dca8c8-f12d-4f0c-96d3-b311f71c1ede)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DeiT](https://arxiv.org/abs/2012.12877v2)</h3> .. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/e25cdddf-f7c2-4880-9837-7a5ce2e91413)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/10.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [ViT](https://arxiv.org/abs/2010.11929) </h3> Introducing ViT (Vision Transformer), an adaptation of the transformer for computer vision applications (image classification in this paper). While this is not the first paper to use transformers for computer vision (see DETR), it is the first work to completely discard CNNs. With ViT, an image is split into fixed-size patches, each of which is linearly embedded and combined with (learnable) positional embedding. The resulting sequence is then fed into a transformer encoder. For classification, an extra learnable classification token is added to the sequence. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/6709e2f5-67e4-47ca-9c38-80b719f91310)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DDPM](https://arxiv.org/abs/2006.11239)</h3> DDPM involves mapping a complex distribution of images into a simpler distribution (Gaussian noise) through a T-steps forward process, and then mapping it back through the backward process. The forward process is defined as Markov chain with Gaussian transition, causing also the backward process to have the same structure. The forward process is fixed, and we learn an approximation of the backward process. To learn the backward process, we maximize the log-likelihood of the data. As in VAE, we actually compute a lower bound and optimize for that. If we are able to create such a bridge (mapping) between the two distributions, then we can sample random noise and go to the original complex distribution to obtain an image. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/b9f660c7-351b-429a-b9d2-4196bfc54c40)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2020-blue?logo=date&style=flat-square)</sub> &thinsp; [DETR](https://arxiv.org/abs/2005.12872)</h3> Introducing DETR (DEtection TRansformers) for object detection (and panoptic segmentation). DETR incorporates CNNs to initially process the input image and extract a *HxWxC* feature tensor. These features are seen as a sequence of *1x1xC* features, which are then passed to the transformer model. The transformer generates a sequence of box predictions and class probabilities. Boxes can be set as empty (no need for non-max suppression at the end). To align the output of the model with the ground truths, a matching loss is used. Moreover, the model optimizes the IoU between predicted and gt boxes, and also the exact number of predictions (the model is penalized if it predicts less empty boxes than expected). <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/1b0665b6-9e19-4fe9-9b5c-e129f7d4fddb)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/03.2017-blue?logo=date&style=flat-square)</sub> &thinsp; [Mask R-CNN](https://arxiv.org/abs/1703.06870) </h3> Mask R-CNN is an extension of Faster R-CNN that incorporates a pixel-level segmentation task alongside object detection. It introduces an additional branch in the network that predicts a (binary) segmentation masks for each detected object. Since the backbone network is shared for both tasks, Mask R-CNN efficiently produces accurate object masks while maintaining real-time performance. It enables precise instance segmentation, distinguishing between different object instances within the same class.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/cca1fdb5-50a5-4b6e-9067-97f9c593ebf1)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [SSD](https://arxiv.org/abs/1512.02325) </h3> Introduces single shot multiBox detector (SSD), a single-stage approach for object detection. The idea of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. To achieve high accuracy, SSD produces predictions from features maps of different scales.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/d3364d52-c53f-4e95-a663-3d9d71fd02d5)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [YOLO](https://arxiv.org/abs/1506.02640)</h3>Presents YOLO, a single-stage appraoch for real-time object detection. YOLO places a grid over the input image and makes each cell responsible for predicting (1) _B_ bounding boxes, each associated with a confidence of the presence of an object within the boxes, (2) a conditional class probability. The bounding boxes are then associated with the class probabilities. A threshold is applied to select only the most confident boxes.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/73e8078c-035f-4bfa-bda8-b95eb8432f27)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [Faster R-CNN](https://arxiv.org/abs/1506.01497) </h3> In R-CNN, region of interest (RoIs) are generated using selective search and processed individually by a CNN. Fast R-CNN improves upon this by processing the entire image just once through the CNN, but still uses selective search. Faster R-CNN takes a step further by directly generating RoIs from the features produced by the CNN. Once the RoIs are generated, the subsequent steps remain the same as in Fast R-CNN. Even if it uses the same features, the method remains a two-stages method.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/c98d9bb9-c8ce-4331-8c7d-ffdc9dde4fbd)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/05.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [U-Net](https://arxiv.org/abs/1505.04597) </h3>Introduces a new architecture for semantic segmentation. U-Net consists of a contracting path (standard convolutional network) followed by an expansive path. The expansive path involves upsampling of the feature map ("up-convolutions") and concatenating it with the corresponding feature map from the contracting path. Combining high resolution features from the contracting path with the upsampled output makes possible to generate very detailed segmentations.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/a82810e1-4d9c-4c85-8df7-8b4382330dae)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/04.2015-blue?logo=date&style=flat-square)</sub> &thinsp; [Fast R-CNN](https://arxiv.org/abs/1504.08083)</h3> Fast R-CNN is a significant improvement over the original R-CNN. R-CNN processes each region of interest (RoI) independently. Fast R-CNN, instead, processes the entire image in a single pass through the CNN. Then, it projects all the RoIs (produced on the original image by an external algorithm) onto the feature space. Each projected RoI is resized through the RoI pooling layer and further processed. A multi-task loss that combines classification and bounding box regression is used (unlike R-CNN, where each component is treated separately).<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/f6984da5-97e7-4e0d-8559-952f1ff3eaa2)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/11.2013-blue?logo=date&style=flat-square)</sub> &thinsp; [R-CNN](https://arxiv.org/abs/1311.2524)</h3> Introduces R-CNN, a combination of region proposals and CNN features for object detection. R-CNN uses a region proposal algorithm (selective search) to first select some regions of interest. Each candidate region is resized to a fixed size and passed to the CNN, which then outputs a feature vector. The feature vector is fed into a collection of class-specific SVMs. Each SVM classifies whether the region (represented by the feature vector) contains an object of a specific class or not.<br><br>|
|||
|||
|||
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/50097be1-1443-4b4c-9d8a-8b598d909a2a)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2017-red?logo=date&style=flat-square) </sub> &thinsp; [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Transformer)</h3> Introduces a novel architecture for sequence-to-sequence tasks. Transformer relies exclusively on self-attention mechanisms, which enables each position in the input sequence to attend to all others simultaneously. This approach facilitates efficient capture of long-range dependencies and parallel computation. It achieves state-of-the-art performance in tasks like machine translation and text generation, setting a new standard for sequence modeling. <br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/b1d4a7d6-114f-429c-ae00-2467c4cd05db)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/06.2014-red?logo=date&style=flat-square) </sub> &thinsp; [GAN](https://arxiv.org/abs/1406.2661v1)</h3> Introduces generative models trained via adversarial process. A generative model G captures the data distribution, while a discriminative model D estimates the probability that a given sample comes from the original data rather than being generated by G. The goal of G is to generate such good samples that D is fooled.<br><br>|
|![image](https://github.com/paulinamoskwa/ml-papers/assets/104844027/28953bb3-69fe-46ef-8a6e-eb6724ca8da7)|<h3> &thinsp; <sub>![](https://img.shields.io/badge/12.2013-red?logo=date&style=flat-square) </sub> &thinsp; [VAE](https://arxiv.org/abs/1312.6114) </h3> Introduces variational autoencoders (VAEs), a class of generative models that extend the traditional autoencoder (AE) by incorporating probabilistic inference. While in AEs the encoder maps the input into a latent space vector, in VAEs the encoder maps the input into a latent space distribution. The decoder samples from the latent distribution and reconstructs the input. VAEs minimize the same loss function as AEs, with an additional regularization term that encourages the latent space to follow a prior distribution, typically a Gaussian.<br><br>|



